{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "975b11d8",
   "metadata": {},
   "source": [
    " ##### In the terminology of machine learning, the dataset is called a training dataset or training set, and each row (containing the data corresponding to one sale) is called an example (or data point, instance, sample). The thing we are trying to predict (price) is called a label (or target). The variables (age and area) upon which the predictions are based are called features (or covariates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a094973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da43879c",
   "metadata": {},
   "source": [
    "##### In a linear regression there are several assumptions. The first being that the realtionship between the fatures $\\bm{x}$ and target $\\bm{y}$ is approximately linear,i.e., that the condiotnal mean $E[Y \\mid X=x]$ can be expressed as a weighted sum of the features $\\bm{x}$. This setup allows that the target value may still deviate from its expected value on account of observation noise. In general we impose the assumption that the noise is wel behaved i.e. has a Gaussian distribution.  \n",
    "\n",
    "The assumption of linearity means that the expectde value of the target can be expressed as a weighted sum of the features (area and age):\n",
    "\n",
    "<a id=\"eq1\"></a>\n",
    "$$\n",
    "price = w_{area} * area + w_{age} * age + b\n",
    "$$\n",
    "\n",
    "where, $w_{area}$ and $w_{age}$ are the corresponding weights and $b$ is the bias. The weights express the influence of each feature on our prediction. \n",
    "\n",
    "#### <font color='red'>The above [equation 1](#eq1) is an affine transformation of the input features, which is characterized by a linear transformation of features via a weighted sum, combined with a translation via the added bias.</font>  \n",
    "\n",
    "#### <font color='lightgreen'>So the goal is to choose the weights \"w\" and the bias \"b\" such that on average, we can make our models predictions fit the true prices observed in the data as closely as possible. </font>\n",
    "\n",
    "In machine learning we usually work with datasets with higher dimensions so it is more convinient to apply compact linear algebra notation. Assuming our datapoints have $d$-dimensions, we can express our prediction $\\bm{\\hat{y}}$ as,\n",
    "\n",
    "<a id=\"eq2\"></a>\n",
    "$$\n",
    "\\hat{y} = w_1x_1 + w_2x_2 + \\dots +w_dx_d + b  \n",
    "$$\n",
    "\n",
    "The features can be clubbed into a single vector $\\bm{x} \\in R^d $ and all weights into a vector $\\bm{w} \\in R^d$ and thus we can express our model compactly via the dot product between $\\bm{w}$ and $\\bm{x}$:\n",
    "\n",
    "<a id=\"eq3\"></a>\n",
    "$$\n",
    "\\bm{\\hat{y}} = \\bm{w}^T\\bm{x} + b\n",
    "$$\n",
    "\n",
    "So when we consider a dataset with n examples and d features we can represent the $\\text{design matrix}$ as $\\mathbf{X} \\in R^{n \\times d}$. The predictions $\\mathbf{\\hat{y}} \\in R^n$ can be expressed as a matrix-vector product:\n",
    "\n",
    "$$\n",
    "\n",
    "\\hat{y}  = \\mathbf{Xw} + b,\n",
    "$$\n",
    "\n",
    "Given features of a training dataset $\\mathbf{X}$ and corresponding (known) labels $\\mathbf{y}$, the goal of linear regression is to find the weight vector $\\mathbf{w}$ and the bias term $b$ such that, given features of a new data example sampled from the same distribution as $\\mathbf{X}$, the new exampleâ€™s label will (in expectation) be predicted with the smallest error.\n",
    "\n",
    "#### <font color='red'> Before we can go about searching for the best parameters (or model parameters) the weights \"w\" and the bias \"b\" and , we will need two more things: (i) a measure of the quality of some given model; and (ii) a procedure for updating the model to improve its quality. </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9cf88f",
   "metadata": {},
   "source": [
    "# Loss Function\n",
    "\n",
    "Loss functions quantify the distance between the real and the predicted values of the target. Loss will be usually a non-negative number where smaller values are better. For regression problems the most common loss function is the sqaured error loss. For an example $i$, the sqaured error between the predicted and the true label is given by,\n",
    "\n",
    "$$\n",
    "l^i(\\bm{w},b) = \\frac{1}{2}(\\hat{y}^i-y^i)^2\n",
    "$$\n",
    "\n",
    "where, \n",
    "\n",
    "$$\n",
    "\\hat{y}^i = w^Tx^i+b\n",
    "$$\n",
    "\n",
    "Note that large differences between estimates $\\hat{y}^i$ and targets $y^i$ lead to even larger contributions to the loss, due to its quadratic form (this quadraticity can be a double-edge sword; while it encourages the model to avoid large errors it can also lead to excessive sensitivity to anomalous data).  \n",
    "\n",
    "To measure the quality of a model on the entire dataset of n examples we simply average the losses on the training set:\n",
    "\n",
    "$$\n",
    "L(\\bm{w},b) = \\frac{1}{n}\\sum_{i=1}^{n}l^i(\\bm{w},b) = \\frac{1}{n}\\sum_{i=1}^{n}\\frac{1}{2}(\\bm{w}^Tx^i + b -y^i)^2\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c6156d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
