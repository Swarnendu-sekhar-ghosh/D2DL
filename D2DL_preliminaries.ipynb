{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80b1d452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d094b124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(12,dtype=torch.float32)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6716f516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01b37bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7c06294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = x.reshape(3,4)\n",
    "X\n",
    "#X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fcd5e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Practitioners often need to work with tensors initialized to contain all 0s or 1s. We can construct a tensor with all elements \n",
    "# set to 0 and a shape of (2, 3, 4) via the zeros function.\n",
    "\n",
    "torch.zeros((2, 3, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fa6dd60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((2, 3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "737c038d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0449,  0.0485,  0.5062, -0.4122],\n",
       "        [ 0.5164, -1.0510,  1.5647,  0.0457],\n",
       "        [ 1.8876,  0.5338,  0.5709,  0.7401]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a86e3e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 1, 4, 3],\n",
       "        [1, 2, 3, 4],\n",
       "        [4, 3, 2, 1]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6968dce0",
   "metadata": {},
   "source": [
    "# Indexing and slicing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cad80e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 3, 2, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7d79cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  1,  4,  3],\n",
       "        [ 1,  2,  3,  4],\n",
       "        [ 4,  3,  2, 18]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[2,3] = 18\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75662e81",
   "metadata": {},
   "source": [
    "**For instance, [:2, :] accesses the first and second rows, where : takes all the elements along axis 1 (column). While we discussed indexing for matrices, this also works for vectors and for tensors of more than two dimensions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6735bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 1, 4, 3],\n",
       "        [1, 2, 3, 4]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e2e4d5a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12, 12,  4,  3],\n",
       "        [12, 12,  3,  4],\n",
       "        [ 4,  3,  2, 18]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:2,:2] = 12\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a990c5ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3067,  0.6179,  0.0470, -0.5577],\n",
       "        [ 1.1591,  0.5375, -0.0422, -1.4697],\n",
       "        [-0.2401, -0.5157, -0.5271, -0.6760],\n",
       "        [ 1.6264,  0.4561,  0.7921, -0.6918]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.randn((4,4))\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9dc2da",
   "metadata": {},
   "source": [
    "# Operations\n",
    "\n",
    "Various mathematical operations can be operated on tensors. Among the most useful of these are the elementwise operations. These apply a standard scalar operation to each element of a tensor. For functions that take two tensors as inputs, elementwise operations apply some standard binary operator on each pair of corresponding elements. We can create an elementwise function from any function that maps from a scalar to a scalar.\n",
    "\n",
    "The mathematical notation for such unary scalar operators is $f \\rightarrow R:R$. This means that the function maps from any real number onto some other real number. Unary functions such as $e^x$ can be applied elementwise.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4759b703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3589, 1.8550, 1.0481, 0.5725],\n",
       "        [3.1871, 1.7118, 0.9587, 0.2300],\n",
       "        [0.7865, 0.5971, 0.5903, 0.5087],\n",
       "        [5.0853, 1.5779, 2.2081, 0.5007]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f191d9",
   "metadata": {},
   "source": [
    "Likewise, we denote binary scalar operators, which map pairs of real numbers to a (single) real number via the signature $f: R, R\\rightarrow R$. Given any two vectors $u$ and $v$ of the same shape, and a binary operator $f$, we can produce a vector $\\mathbf{c} = F(u,v)$ by setting $c_i \\leftarrow f(u_i,v_i)$ for all $i$, where $c_i$,$u_i$, and $v_i$ are the $i^{th}$ elements of vectors $\\mathbf{c}$, $\\mathbf{u}$, and $\\mathbf{v}$. \n",
    "\n",
    "Here, we produced the vector-valued $\\mathbf{F} : R^d, R^d \\rightarrow R^d$ by lifting the scalar function to an elementwise vector operation. The common standard arithmetic operators for addition (+), subtraction (-), multiplication (*), division (/), and exponentiation (**) have all been lifted to elementwise operations for identically-shaped tensors of arbitrary shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "578c6ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([12, 14, 16, 18, 20, 22]),\n",
       " tensor([-10, -10, -10, -10, -10, -10]),\n",
       " tensor([11, 24, 39, 56, 75, 96]),\n",
       " tensor([0.0909, 0.1667, 0.2308, 0.2857, 0.3333, 0.3750]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.tensor([1,2,3,4,5,6])\n",
    "n = torch.tensor([11,12,13,14,15,16])\n",
    "\n",
    "m+n, m-n, m*n, m/n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f5e9fb",
   "metadata": {},
   "source": [
    "##### We can also concatenate multiple tensors, stacking them end-to-end to form a larger one. We just need to provide a list of tensors and tell the system along which axis to concatenate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1537948e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]]),\n",
       " tensor([[ 1,  2,  3,  4],\n",
       "         [45, 96, 13, 24],\n",
       "         [ 4,  3,  2,  1]]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = torch.arange(12,dtype=torch.float32).reshape((3,4))\n",
    "N = torch.tensor([[1,2,3,4],[45,96,13,24],[4,3,2,1]])\n",
    "M,N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "99898eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [ 1.,  2.,  3.,  4.],\n",
       "         [45., 96., 13., 24.],\n",
       "         [ 4.,  3.,  2.,  1.]]),\n",
       " tensor([[ 0.,  1.,  2.,  3.,  1.,  2.,  3.,  4.],\n",
       "         [ 4.,  5.,  6.,  7., 45., 96., 13., 24.],\n",
       "         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((M,N),dim=0), torch.cat((M,N),dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115362aa",
   "metadata": {},
   "source": [
    "##### Sometimes, we want to construct a binary tensor via logical statements. Take M == N as an example. For each position i, j, if M[i, j] and N[i, j] are equal, then the corresponding entry in the result takes value 1, otherwise it takes value 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d7a48e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M==N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dba22e7",
   "metadata": {},
   "source": [
    "##### Summing all the elements in the tensor yields a tensor with only one element.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f2f020c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(66.), tensor(81))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.sum(),n.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39be22fa",
   "metadata": {},
   "source": [
    "# Broadcasting\n",
    "\n",
    "We can still perform elementwise binary operations by invoking the broadcasting mechanism. Broadcasting works according to the following two-step procedure: (i) expand one or both arrays by copying elements along axes with length 1 so that after this transformation, the two tensors have the same shape; (ii) perform an elementwise operation on the resulting arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6b0ab206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0],\n",
       "         [1],\n",
       "         [2]]),\n",
       " tensor([[0, 1]]),\n",
       " tensor([[ 1.0000,  2.7183],\n",
       "         [ 2.7183,  7.3891],\n",
       "         [ 7.3891, 20.0855]]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.arange(3).reshape((3,1))\n",
    "c = torch.arange(2).reshape((1,2))\n",
    "d = torch.exp(b+c)\n",
    "b,c,d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f26380",
   "metadata": {},
   "source": [
    "# Saving Memory\n",
    "\n",
    "if we write Y = X + Y, we dereference the tensor that Y used to point to and instead point Y at the newly allocated memory. We can demonstrate this issue with Python’s id() function, which gives us the exact address of the referenced object in memory. Note that after we run Y = Y + X, id(Y) points to a different location. That is because Python first evaluates Y + X, allocating new memory for the result and then points Y to this new location in memory.\n",
    "\n",
    "This might be undesirable for two reasons. First, we do not want to run around allocating memory unnecessarily all the time. In machine learning, we often have hundreds of megabytes of parameters and update all of them multiple times per second. Whenever possible, we want to perform these updates in place. Second, we might point at the same parameters from multiple variables. If we do not update in place, we must be careful to update all of these references, lest we spring a memory leak or inadvertently refer to stale parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f868a7ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(d)\n",
    "d = 2*a+b\n",
    "id(d) == before "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7087ba7b",
   "metadata": {},
   "source": [
    "##### Fortunately, performing in-place operations is easy. We can assign the result of an operation to a previously allocated array \"e\" by using slice notation: e[:] = **expression**. To illustrate this concept, we overwrite the values of tensor e, after initializing it, using zeros_like, to have the same shape as b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9e335630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11],\n",
       "         [12, 13, 14, 15]]),\n",
       " tensor([[ 1,  2,  3,  4],\n",
       "         [ 5,  6,  7,  8],\n",
       "         [ 9, 10, 11, 12],\n",
       "         [13, 14, 15, 16]]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(16).reshape(4,4)\n",
    "B = torch.tensor([[1,2,3,4], [5,6,7,8], [9,10,11,12], [13,14,15,16]]).reshape((4,4))\n",
    "A,B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e97a59ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 4]),\n",
       " 2539006161488,\n",
       " tensor([[ 1,  3,  5,  7],\n",
       "         [ 9, 11, 13, 15],\n",
       "         [17, 19, 21, 23],\n",
       "         [25, 27, 29, 31]]),\n",
       " True)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = torch.zeros_like(B)\n",
    "#e.size()\n",
    "e_before = id(e)\n",
    "#e_before\n",
    "e[:] = A+B\n",
    "#e[:]\n",
    "e.size(),e_before,e[:],id(e) == e_before\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44f0cad",
   "metadata": {},
   "source": [
    "##### If the value of A is not reused in subsequent computations, we can also use A[:] = A + B or  A+= B to reduce the memory overhead of the operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f2bfd20d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2539006119920,\n",
       " tensor([[ 2,  5,  8, 11],\n",
       "         [14, 17, 20, 23],\n",
       "         [26, 29, 32, 35],\n",
       "         [38, 41, 44, 47]]),\n",
       " 2539006119920,\n",
       " True)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_before = id(A)\n",
    "A_before\n",
    "A+=B\n",
    "A_after = id(A)\n",
    "A_before, A , A_after, A_before == A_after"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c02a1cf",
   "metadata": {},
   "source": [
    "# Conversion to Other Python Objects"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
